{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4559177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import sql_queries\n",
    "import psycopg2\n",
    "import create_tables\n",
    "import etl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a841d",
   "metadata": {},
   "source": [
    "### I read the config file again to access and interact with the Redshift Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671aa095",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('project_dwh.cfg'))\n",
    "\n",
    "DWH_IAM_ROLE_NAME = config.get('IAM_ROLE','DWH_IAM_ROLE_NAME')\n",
    "\n",
    "DB_NAME = config.get('CLUSTER','DB_NAME')\n",
    "HOST = config.get('CLUSTER','HOST')\n",
    "DB_USER = config.get('CLUSTER','DB_USER')\n",
    "DB_PASSWORD = config.get('CLUSTER','DB_PASSWORD')\n",
    "DB_PORT = config.get('CLUSTER','DB_PORT')\n",
    "DWH_ENDPOINT = config.get('CLUSTER','DWH_ENDPOINT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960cef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tables.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc381470",
   "metadata": {},
   "source": [
    "### I initiated the connection to the Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(DWH_ENDPOINT,DB_NAME,DB_USER,DB_PASSWORD,DB_PORT))\n",
    "cur = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb8c82",
   "metadata": {},
   "source": [
    "### Here I drop and recreate the necessary tables: the 2 staging tables and other star schema tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c93d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tables.drop_tables(cur,conn)\n",
    "create_tables.create_tables(cur,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9950467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29396403",
   "metadata": {},
   "source": [
    "### Here I load the 2 staging tables with the data from S3 buckets.\n",
    "#### To save time, I only used the files in the A/A/ folder of the song_data bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd5ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "etl.load_staging_tables(cur,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba63a21",
   "metadata": {},
   "source": [
    "### The data in the staging tables are loaded to the production tables inside the Redshift Cluster.\n",
    "#### Since, this is a SQL-to-SQL ETl, I used the combination of Insert Into and Select statements. We see which queries are ran without any error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21acb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "etl.insert_tables(cur,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee71c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
